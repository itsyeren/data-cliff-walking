{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c22d8c3e",
      "metadata": {},
      "source": [
        "# ‚õ∞Ô∏è Cliff Walking\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9377683e",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "In this challenge, you will learn the fundamental aspects of using Gymnasium and Stable Baselines3 to train reinforcement learning models. This challenge is designed to give you a comprehensive understanding of setting up environments, training models, visualizing their performance, and finally using a trained model to interact with an environment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d3f4176",
      "metadata": {},
      "source": [
        "---\n",
        "It's a good practice to place all your import statements at the top of your notebook to make it easier to rerun your notebook.\n",
        "\n",
        "Throughout the challenges, you will need to add import statements. Add them all to the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [
          "challengify"
        ]
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "674e34ac",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### üéØ Challenge Objectives\n",
        "\n",
        "#### üèãÔ∏è‚Äç‚ôÄÔ∏è Gain Familiarity with Gymnasium:\n",
        "We will start by exploring the [CliffWalking environment](https://gymnasium.farama.org/environments/toy_text/cliff_walking/) from Gymnasium. Understand the fundamental methods used to manage and visualize environments. Learn to load and reset environments, take actions, and visualize results.\n",
        "\n",
        "#### ü§ñ Learn to Use Stable Baselines3:\n",
        "Use the Stable Baselines3 library to set up and train a reinforcement learning model. You will learn to configure a model, adjust training parameters, and start the training process using a popular algorithm like Deep Q-Network (DQN).\n",
        "\n",
        "#### üìà Visualize Training Performance:\n",
        "Apply visualization tools such as logging and TensorBoard to monitor and plot your model's training performance. Analyze metrics such as rewards per episode and learning curves to evaluate the effectiveness of your training strategies.\n",
        "\n",
        "#### üíæ Load and Deploy a Trained Model:\n",
        "After training, learn to save a trained model and load it later. Use this model to run a simulation where the agent interacts with the CliffWalking environment and apply the learned policies to navigate the environment effectively.\n",
        "\n",
        "#### üîç Evaluate and Reflect:\n",
        "Evaluate the trained model's performance in real-time interactions within the environment. Reflect on the learning process and the agent's behavior, understanding how different configurations and training durations can affect the results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a227d54",
      "metadata": {},
      "source": [
        "---\n",
        "### Section 1: Exploring the CliffWalking Environment\n",
        "\n",
        "In this first task, you will set up the [CliffWalking environment](https://gymnasium.farama.org/environments/toy_text/cliff_walking/) from Gymnasium.\n",
        "\n",
        "#### üìù Steps to Follow:\n",
        "\n",
        "#### 0. ‚öôÔ∏è Import the Package:\n",
        "Import gymnasium in the cell at the top of your notebook. Make sure we can use it as `gym` in our code.\n",
        "\n",
        "#### 1. üóÇÔ∏è Load the Environment:\n",
        "- Use the `.make()` method to load the `CliffWalking` environment.  \n",
        "- Set `render_mode` appropriately to visualize the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de685798",
      "metadata": {},
      "source": [
        "Note: The `gymnasium` documentation tells you to use `CliffWalking-v1`, but this is not yet available in the `gymnasium` version 1.0.0 we are using for compatibility reasons. Instead, use `CliffWalking-v0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make('CliffWalking-v0', render_mode='human')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "358c4b8e",
      "metadata": {},
      "source": [
        "#### 2. üîÑ Initialize the Environment:\n",
        "Call the `.reset()` method to bring the environment to its initial state ‚Äî this is required before interacting with it. The `.reset()` method returns a state: the initial state. Store this in a variable and print it.\n",
        "\n",
        "> You may see a `UserWarning: pkg_resources is deprecated ...` warning. You can ignore this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": [
          "challengify"
        ]
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/yaren/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import resource_stream, resource_exists\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(36, {'prob': 1})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "initial_state = env.reset()\n",
        "initial_state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a77b0b44",
      "metadata": {},
      "source": [
        "The output `(36, {'prob': 1})` provides two pieces of information about the environment's state:\n",
        "\n",
        "**üî¢ State Index (36)**: This number represents the agent's specific position within the environment grid at the start. For example, in the 'CliffWalking-v0' environment, the index `36` corresponds to the initial state where the agent starts the episode.\n",
        "\n",
        "**üé≤ Probability Information ({'prob': 1})**: This dictionary shows additional details about the state, particularly regarding transition probability. The value `1` for the `'prob'` key indicates that the transition to this state occurred with probability 1, meaning it is certain. This makes sense because the agent must start in this state."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ef7a97b",
      "metadata": {},
      "source": [
        "#### 3. üëÄ Visualize the Environment:\n",
        "Use the `.render()` method to display the environment and understand its layout before taking any action.\n",
        "\n",
        "Depending on your setup, you may not need this step as the new state may already be rendered after the previous step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [
          "challengify"
        ]
      },
      "outputs": [],
      "source": [
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e06c5ff6",
      "metadata": {},
      "source": [
        "#### 4. üßπ Close the Environment:\n",
        "Properly close the environment by calling `.close()` to free up resources when you're done."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f9fa624",
      "metadata": {},
      "source": [
        "<details>\n",
        "  <summary markdown='span'>‚ö†Ô∏è <strong>Note about closing on macOS</strong></summary>\n",
        "  \n",
        "  `.close()` will not close the environment window. This is fine, **you can leave it open**. The window will be reused in the next steps.\n",
        "\n",
        "  At the end of the challenge and **only at the end**, you will need to force quit it yourself to close the environment window:\n",
        "  1. Find the window and click the red button to close it. (This will fail.)\n",
        "  1. Click the Apple symbol in the top left corner of your screen.\n",
        "  1. Select `Force Quit`.\n",
        "  1. Find the `python (Not Responding)` process in the process list. Select it.\n",
        "  1. Click the `Force Quit` button.\n",
        "\n",
        "  Since this will terminate your kernel, **only do this at the end of the challenge**.\n",
        "\n",
        "  This happens because we're running inside Jupyter Notebook. If you move your code to a `.py` file, you won't see this behavior.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": [
          "challengify"
        ]
      },
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c39cd1fc",
      "metadata": {},
      "source": [
        "---\n",
        "### Section 2: Interacting with the Environment\n",
        "\n",
        "Now let's load the environment again, take a random step, and show the result. This will introduce the fundamental methods for interacting with environments ‚Äî helping you understand how agents move and receive feedback."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b659512",
      "metadata": {},
      "source": [
        "#### 1. üóÇÔ∏è Load the Environment\n",
        "- Load the environment.\n",
        "- Reset it to the initial state.\n",
        "- Print the initial state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": [
          "challengify"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial state: (36, {'prob': 1})\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('CliffWalking-v0', render_mode='human')\n",
        "initial_state = env.reset()\n",
        "print(\"Initial state:\", initial_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1f71f76",
      "metadata": {},
      "source": [
        "#### 2. üé≤ Sample an Action:\n",
        "- Use `.action_space.sample()` to select a random action from the environment's action space ‚Äî simulates exploration.\n",
        "- Store this in a variable.\n",
        "- Examine the type and value of the action.\n",
        "- Run the cell several times. What values do you see? What do they represent?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "tags": [
          "challengify"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type of action: <class 'numpy.int64'>\n",
            "Value of action: 1\n"
          ]
        }
      ],
      "source": [
        "action = env.action_space.sample()\n",
        "print(\"Type of action:\", type(action))\n",
        "print(\"Value of action:\", action)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc7f50b5",
      "metadata": {},
      "source": [
        "#### 2. ü¶∂ Execute the Action:\n",
        "- Apply the action using `.step()`.  \n",
        "- This returns:  \n",
        "  - New state  \n",
        "  - Reward  \n",
        "  - A `done` flag (whether the episode has ended or not)  \n",
        "  - Additional information (if any)\n",
        "- Use \"tuple unpacking\" to store each of these returned values in a variable. Print them. Do you understand the new state, reward, and done return values?\n",
        "- Run this **reset > action > step** sequence several times and check the different results.\n",
        "\n",
        "<details>\n",
        "  <summary markdown='span'>\n",
        "  üí° Tuple unpacking?\n",
        "  </summary>\n",
        "\n",
        "  If a function returns a tuple, you can immediately store the different elements of the tuple in different variables.\n",
        "\n",
        "  Example:\n",
        "\n",
        "  Suppose you have this function:\n",
        "\n",
        "  ```python\n",
        "  def surface_and_circumference(a, b):\n",
        "    \"\"\"Returns the area and perimeter of a rectangle \n",
        "    with length `a` and width `b`.\"\"\"\n",
        "    return a*b, 2*a + 2*b\n",
        "  ```\n",
        "\n",
        "  Instead of:\n",
        "\n",
        "  ```python\n",
        "  result = surface_and_circumference(4, 2)\n",
        "  surface = result[0]\n",
        "  circumference = result[1]\n",
        "  ```\n",
        "\n",
        "  You can immediately do:\n",
        "\n",
        "  ```python\n",
        "  surface, circumference = surface_and_circumference(4, 2)\n",
        "  ```\n",
        "\n",
        "  If you're only going to use surface in the rest of your code, it's common practice to use `_` for the other return values. This signals to other programmers that you're discarding the remaining values.\n",
        "\n",
        "  Example:\n",
        "  \n",
        "  ```python\n",
        "  surface, _ = surface_and_circumference(4, 2)\n",
        "  # Code that only needs surface comes after this\n",
        "  ```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "tags": [
          "challengify"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New state: 36\n",
            "Reward: -100\n",
            "Done: False\n"
          ]
        }
      ],
      "source": [
        "new_state, reward, done, info, _ = env.step(action)\n",
        "print(\"New state:\", new_state)\n",
        "print(\"Reward:\", reward)\n",
        "print(\"Done:\", done)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "376e87e5",
      "metadata": {},
      "source": [
        "**üìç New State**: This number represents the agent's state after performing the specified action. In the `CliffWalking` environment, the state corresponds to a specific position in the environment grid where the agent has moved.\n",
        "\n",
        "**üí∏ Reward**: The reward value shows the immediate feedback given by the environment as a result of the agent's action. In many grid-based environments, a negative reward like this typically represents a penalty and indicates that the action taken may not be optimal or is being penalized to encourage other strategies.\n",
        "\n",
        "**üö¶ Done**: The boolean value indicates whether the episode has ended. In this case, `False` means the episode is still ongoing and the agent has not reached a terminal state (such as the goal or a cliff) that would end the episode."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b203ec8a",
      "metadata": {},
      "source": [
        "#### 3. üëÅÔ∏è Visualize the Result:\n",
        "- Call `.render()` again to visualize the environment after the action ‚Äî and see how the state has changed.\n",
        "- If you don't see the render window, it's probably hidden behind your other windows or displayed on another desktop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "tags": [
          "challengify"
        ]
      },
      "outputs": [],
      "source": [
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18134543",
      "metadata": {},
      "source": [
        "#### 4. üßπ Close the Environment:\n",
        "Properly close the environment by calling `.close()` to free up resources when you're done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "tags": [
          "challengify"
        ]
      },
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "402e3e80",
      "metadata": {},
      "source": [
        "---\n",
        "### Section 3: Directed Interaction with the Environment\n",
        "\n",
        "This time, instead of selecting a random action, you will take an intentional step ‚Äî we will move **UP**. This reinforces the idea of purposeful decision-making in reinforcement learning.\n",
        "\n",
        "#### 1. üöÄ Initialize and Print the Initial State:\n",
        "- Load the environment and call `.reset()` to get the initial state.  \n",
        "- Use `print()` to display the initial state before taking any action.\n",
        "- You should get this output `(36, {'prob': 1})`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [
          "steps"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial state: (36, {'prob': 1})\n"
          ]
        }
      ],
      "source": [
        "# Load the environment again, and initialize it\n",
        "\n",
        "env = gym.make('CliffWalking-v0', render_mode='human')\n",
        "\n",
        "# Reset the environment to the initial state\n",
        "\n",
        "initial_state = env.reset()\n",
        "\n",
        "# Print the initial state\n",
        "\n",
        "print(\"Initial state:\", initial_state)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3608246",
      "metadata": {},
      "source": [
        "#### 2. ‚¨ÜÔ∏è Determine and Execute the 'UP' Action\n",
        "\n",
        "- Check `.action_space` and the documentation to find the index for the **'UP'** action.  \n",
        "- Use `.step(action_index)` to execute this action.  \n",
        "- Print the result to see:\n",
        "  - New state  \n",
        "  - Reward received  \n",
        "  - Whether the episode has ended (`done` flag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [
          "steps"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New state: 24\n",
            "Reward: -1\n",
            "Done: False\n"
          ]
        }
      ],
      "source": [
        "# Take a step to move 'UP'\n",
        "\n",
        "action = 0  # Action 0 corresponds to 'UP'\n",
        "\n",
        "# Take the action\n",
        "\n",
        "new_state, reward, done, info, _ = env.step(action)\n",
        "\n",
        "# Print the new state, reward, and done status\n",
        "\n",
        "print(\"New state:\", new_state)\n",
        "print(\"Reward:\", reward)\n",
        "print(\"Done:\", done)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "938e1303",
      "metadata": {},
      "source": [
        "#### 3. üñºÔ∏è Visualize and Close the Environment\n",
        "\n",
        "- Use `.render()` to visualize the environment after taking the action ‚Äî see how the state has changed.\n",
        "- Then call `.close()` to properly close the environment and free up system resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [
          "steps"
        ]
      },
      "outputs": [],
      "source": [
        "# Render the environment after taking a step\n",
        "\n",
        "env.render()\n",
        "\n",
        "# Close the environment\n",
        "\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "159f66dc",
      "metadata": {},
      "source": [
        "---\n",
        "### Section 4: Training an RL Algorithm\n",
        "\n",
        "First, you will prepare the `CliffWalking-v0` environment for training with Stable Baselines3 models. This involves proper setup and wrapping to ensure compatibility with RL algorithms.\n",
        "\n",
        "#### üß± 1. Load the Environment:\n",
        "- Use `gym.make()` to create the `CliffWalking-v0` environment.  \n",
        "- Set `render_mode='human'` ‚Üí Provides visual feedback during interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [
          "challengify"
        ]
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CliffWalking-v0\", render_mode=\"human\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6eb41dbe",
      "metadata": {},
      "source": [
        "#### üßµ 2. Wrap the Environment\n",
        "\n",
        "Wrapping your environment with [`DummyVecEnv`](https://stable-baselines3.readthedocs.io/en/v1.0/guide/vec_envs.html#dummyvecenv) is a small but critical step ‚Äî it ensures your RL setup works seamlessly with Stable Baselines3.\n",
        "\n",
        "Go ahead and wrap the environment using [`DummyVecEnv`](https://stable-baselines3.readthedocs.io/en/v1.0/guide/vec_envs.html#dummyvecenv) from Stable Baselines3.  üëá"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "04920008",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Wrap the environment to make it compatible with SB3\n",
        "env = DummyVecEnv([lambda: env])  # Lambda function to ensure proper environment handling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "848171cd",
      "metadata": {},
      "source": [
        "#### üßµ More About `DummyVecEnv`\n",
        "\n",
        "`DummyVecEnv` is a wrapper from Stable Baselines3 that vectorizes Gym environments to make them compatible with RL models.\n",
        "\n",
        "#### ‚öôÔ∏è What Does It Do?\n",
        "\n",
        "- **üì¶ Standardizes the API**  \n",
        "  Ensures the environment matches the expected format for Stable Baselines3 algorithms.\n",
        "\n",
        "- **üìä Enables Batch Processing**  \n",
        "  Even with a single environment, interactions are processed as a batch ‚Äî necessary for scaling later with tools like `SubprocVecEnv`.\n",
        "\n",
        "- **üîó Ensures Compatibility**  \n",
        "  Wraps `reset()` and `step()` to work correctly within training loops."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bd9c6cc",
      "metadata": {},
      "source": [
        "#### 3. Initialize a DQN Model from SB3\n",
        "\n",
        "Now that your environment is ready, you need to initialize the model. In this case, you will configure a Deep Q-Network (DQN) using Stable Baselines3. DQN uses a neural network to predict Q-values ‚Äî predicting the expected reward for each action in a given state.\n",
        "\n",
        "#### üìù Steps to Follow:\n",
        "- Import `DQN` from `stable_baselines3`. Add this to the cell at the top of the notebook.\n",
        "- Initialize an instance of the DQN model with the following parameters:\n",
        "    - Use `'MlpPolicy'` ‚Äî a basic neural network that maps observations to actions.  \n",
        "    - Set `env` to your environment and `verbose=1` for detailed training logs.\n",
        "    - Add the `tensorboard_log` parameter to track training metrics. We will use this later to track training with TensorBoard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [
          "challengify"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "model = DQN(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./dqn_cliff_walking_tensorboard/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fdcd78c",
      "metadata": {},
      "source": [
        "### 4. üèãÔ∏è‚Äç‚ôÇÔ∏è Train and Save the DQN Model\n",
        "\n",
        "Time to train your DQN model and save it for future use ü•≥\n",
        "\n",
        "#### üìù Steps to Follow:\n",
        "\n",
        "#### ‚è±Ô∏è Set Training Step Count\n",
        "- Define how long you will train the model (in terms of number of interactions with the environment).  \n",
        "- For now, use: `total_timesteps = 1000`\n",
        "\n",
        "#### üß† Train the Model\n",
        "- Call `.learn()` on your DQN model to start training.  \n",
        "- The model will improve its policy over time based on feedback.\n",
        "\n",
        "#### üíæ Save the Trained Model\n",
        "- Use `.save()` to save the model to disk.  \n",
        "- Print the file path to verify it was saved correctly.\n",
        "\n",
        "üé• You can follow the steps in the rendered environment during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [
          "steps"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logging to ./dqn_cliff_walking_tensorboard/DQN_2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5d4ec601af84b71a6fd8a5b0bb0d013",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model trained and saved to: ./dqn_cliff_walking\n"
          ]
        }
      ],
      "source": [
        "# Set training timesteps\n",
        "\n",
        "total_timesteps = 1000\n",
        "\n",
        "# Train the model (set `progress_bar=True` to show progress)\n",
        "\n",
        "model.learn(total_timesteps=total_timesteps, progress_bar=True)\n",
        "\n",
        "# Define model save path\n",
        "\n",
        "model_path = \"./dqn_cliff_walking\"\n",
        "\n",
        "# Save the model\n",
        "\n",
        "model.save(model_path)\n",
        "print(\"Model trained and saved to:\", model_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e55a8be",
      "metadata": {},
      "source": [
        "Congratulations, you've trained your first RL model!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e568d2fc",
      "metadata": {},
      "source": [
        "### Section 5: Efficient Training Without Visualization\n",
        "\n",
        "In this section, you will go through the entire process of preparing the environment, loading the model, and training, but without visual rendering. Disabling visualization during the training phase can significantly speed up the training process as it reduces computational overhead.\n",
        "\n",
        "#### üìù Steps to Follow:\n",
        "\n",
        "#### üß± 1. Load and Prepare the Environment\n",
        "- Use `render_mode=None` to disable visual output.\n",
        "\n",
        "#### ‚öôÔ∏è 2. Configure and Initialize the DQN Model\n",
        "- Use `'MlpPolicy'` and `DummyVecEnv` as before.  \n",
        "- Keep `verbose=1` if you want log output.\n",
        "- We will add a TensorBoard logging location `./dqn_tensorboard` so we can track training with TensorBoard.\n",
        "\n",
        "#### ‚è±Ô∏è 3. Set Training Parameters and Train\n",
        "- Increase the step count for better learning (e.g., `total_timesteps = 100_000`).  \n",
        "- Call `.learn()` to start training.\n",
        "\n",
        "#### üíæ 4. Save the Trained Model\n",
        "- Use `.save(\"dqn_cliffwalking_fast\")` to store your model.\n",
        "\n",
        "By following these steps, you will efficiently train and save a DQN model without the additional computational overhead of visualizing the environment. This approach is particularly useful when training complex models or using limited computational resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "tags": [
          "steps"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Logging to dqn_cliff_walking_tensorboard/DQN_3\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[30], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100_000\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Define model save path\u001b[39;00m\n\u001b[1;32m     23\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdqn_cliff_walking_model.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/stable_baselines3/dqn/dqn.py:267\u001b[0m, in \u001b[0;36mDQN.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[1;32m    260\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDQN:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/stable_baselines3/common/off_policy_algorithm.py:328\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 328\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/stable_baselines3/common/off_policy_algorithm.py:560\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    557\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    563\u001b[0m num_collected_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
            "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/gymnasium/envs/toy_text/cliffwalking.py:202\u001b[0m, in \u001b[0;36mCliffWalkingEnv.step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlastaction \u001b[38;5;241m=\u001b[39m a\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(s), r, t, \u001b[38;5;28;01mFalse\u001b[39;00m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprob\u001b[39m\u001b[38;5;124m\"\u001b[39m: p}\n",
            "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/gymnasium/envs/toy_text/cliffwalking.py:228\u001b[0m, in \u001b[0;36mCliffWalkingEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/gymnasium/envs/toy_text/cliffwalking.py:315\u001b[0m, in \u001b[0;36mCliffWalkingEnv._render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    313\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m    314\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# rgb_array\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[1;32m    318\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_surface)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    319\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Load environment\n",
        "\n",
        "env = gym.make(\"CliffWalking-v0\", render_mode=\"human\")\n",
        "\n",
        "# Wrap the environment to be compatible with SB3\n",
        "\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "# Load the model, match it with the environment, set a policy, add a logging location\n",
        "\n",
        "model = DQN(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"dqn_cliff_walking_tensorboard/\")\n",
        "\n",
        "# Set training timesteps\n",
        "\n",
        "timesteps = 100_000\n",
        "\n",
        "# Train the model\n",
        "\n",
        "model.learn(total_timesteps=timesteps)\n",
        "\n",
        "# Define model save path\n",
        "\n",
        "model_path = \"dqn_cliff_walking_model.zip\"\n",
        "\n",
        "# Save the model\n",
        "\n",
        "model.save(model_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "128e22b0",
      "metadata": {},
      "source": [
        "#### üìä Understanding Training Log Metrics\n",
        "\n",
        "When training a reinforcement learning model, various metrics help monitor learning progress and performance.\n",
        "\n",
        "#### üé≤ Rollout Metrics\n",
        "\n",
        "- **exploration_rate** ‚Üí Probability of taking a random action.  \n",
        "  - High = more exploration  \n",
        "  - Low = more exploitation  \n",
        "\n",
        "#### ‚è±Ô∏è Time-Related Metrics\n",
        "\n",
        "- **episodes** ‚Üí Number of completed episodes.  \n",
        "- **fps** ‚Üí Frames per second (how fast training is running).  \n",
        "- **time_elapsed** ‚Üí Total time elapsed since training started (in seconds).  \n",
        "- **total_timesteps** ‚Üí Total number of steps taken in the environment.\n",
        "\n",
        "#### üß† Training Metrics\n",
        "\n",
        "- **learning_rate** ‚Üí Size of updates made to model weights.  \n",
        "  - Low = slower but more stable learning  \n",
        "- **loss** ‚Üí Model's prediction error.  \n",
        "  - Decreasing loss = learning is working  \n",
        "- **n_updates** ‚Üí Number of times the model has updated its weights.\n",
        "\n",
        "#### üîç How to Interpret\n",
        "\n",
        "- **‚¨áÔ∏è exploration_rate** ‚Üí Agent is transitioning from exploration to learned behavior.  \n",
        "- **‚ö° High fps** ‚Üí Training is running efficiently.  \n",
        "- **üìâ Decreasing loss** ‚Üí Model is improving and making fewer errors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdfcdb21",
      "metadata": {},
      "source": [
        "#### üñ•Ô∏è Start TensorBoard\n",
        "\n",
        "TensorBoard allows you to track how training is going. The interactive dashboard shows learning curves, reward trends, and more. \n",
        "\n",
        "To use TensorBoard:\n",
        "\n",
        "1. Open a terminal window.\n",
        "1. Make sure you're in the challenge folder!\n",
        "1. Run this command (you may need to replace it with the path you set during training):\n",
        "   ```bash\n",
        "   tensorboard --logdir=./dqn_cliff_tensorboard/\n",
        "   ```\n",
        "1. Follow the link shown in the terminal (probably `localhost:6006`).\n",
        "\n",
        "You can start TensorBoard while your model is still training: its purpose is to track how training is going. At the start of training, you may get a `No dashboards are active for the current data set.` warning. **Be patient: you won't see anything until the first episode completes.**\n",
        "\n",
        "It's also possible to open TensorBoard inside your notebook:\n",
        "\n",
        "```bash\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=./dqn_cliff_tensorboard/\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa05708c",
      "metadata": {},
      "source": [
        "### üì¶ Section 6: Use a Trained Model\n",
        "\n",
        "Instead of training again from scratch, you can load the model you just trained to quickly observe how your agent behaves.  \n",
        "Alternatively, you can load a more powerful pre-trained model if one is available.\n",
        "\n",
        "#### üì• 1. Load the Pre-trained Model\n",
        "\n",
        "- Use `.load(\"path_to_your_pre_trained_model\")` to load the model from disk.  \n",
        "- Replace the path with the actual location of the model you saved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [
          "challengify"
        ]
      },
      "outputs": [],
      "source": [
        "model = DQN.load(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83b11b28",
      "metadata": {},
      "source": [
        "#### üß± 2. Prepare the Environment\n",
        "\n",
        "Load and reset the environment to start a new episode before running the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [
          "challengify"
        ]
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CliffWalking-v0\", render_mode=\"human\")\n",
        "env = DummyVecEnv([lambda: env])\n",
        "obs = env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6c2997a",
      "metadata": {},
      "source": [
        "#### 3. üß† Predict the Best Next Action\n",
        "\n",
        "Use the model's `.predict()` method to select the next action based on the current observation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [
          "challengify"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action to take: [2]\n"
          ]
        }
      ],
      "source": [
        "action, _states = model.predict(obs, deterministic=True)\n",
        "print(\"Action to take:\", action)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbda070e",
      "metadata": {},
      "source": [
        "#### 4. üö∂‚Äç‚ôÇÔ∏è Take the Step\n",
        "\n",
        "- Use `.step(action)` to apply the predicted action.  \n",
        "- This returns:\n",
        "  - New state  \n",
        "  - Reward  \n",
        "  - A `done` flag (if the episode has ended)  \n",
        "  - Additional information (if any)\n",
        "\n",
        "If your agent makes strange movements or gets stuck, don't panic: it probably hasn't been trained long enough. Go to the end of the challenge before trying to train longer üòâ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [
          "challengify"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n"
          ]
        }
      ],
      "source": [
        "obs, reward, done, info = env.step(action)\n",
        "print(obs, reward, done, info)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08d05fd5",
      "metadata": {},
      "source": [
        "#### 5. üñºÔ∏è Visualize the Environment\n",
        "\n",
        "Use `.render()` to visualize the current state of the environment after the action has been executed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [
          "challengify"
        ]
      },
      "outputs": [],
      "source": [
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4107a27",
      "metadata": {},
      "source": [
        "#### 6.  üßπ Close the Environment\n",
        "\n",
        "Use `.close()` to properly close the environment and free up system resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [
          "challengify"
        ]
      },
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f4afafd",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Section 7: Implement the Full Interaction Loop\n",
        "\n",
        "In this final exercise, you will create a complete loop where your pre-trained model interacts step-by-step with the environment until the episode ends.\n",
        "\n",
        "- Use a `while` loop to repeat actions.\n",
        "- Call `.predict()` to select actions and `.step()` to apply them.\n",
        "- Make sure you feed the new state to the next predict iteration.\n",
        "- Use the `done` flag from `.step()` to know when to exit the loop.\n",
        "- Don't forget to call `.render()` after each step to visualize the agent in action.\n",
        "\n",
        "Again, if your agent selects strange movements or gets stuck in an infinite loop, don't panic: it probably hasn't been trained long enough. If your agent gets stuck in an infinite loop, stop the cell execution.\n",
        "\n",
        "üëâ Go to the end of the challenge before trying to train longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [
          "steps"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n",
            "[36] [-1.] [False] [{'prob': 1.0, 'TimeLimit.truncated': False}]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[45], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(obs, reward, done, info)\n\u001b[0;32m---> 24\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Close the environment\u001b[39;00m\n\u001b[1;32m     28\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:103\u001b[0m, in \u001b[0;36mDummyVecEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Gym environment rendering. If there are multiple environments then\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    they are tiled together in one image via ``BaseVecEnv.render()``.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    :param mode: The rendering type.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:250\u001b[0m, in \u001b[0;36mVecEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# mode == self.render_mode == \"human\"\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# In that case, we try to call `self.env.render()` but it might\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# crash for subprocesses\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# call the render method of the environments\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:129\u001b[0m, in \u001b[0;36mDummyVecEnv.env_method\u001b[0;34m(self, method_name, indices, *method_args, **method_kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call instance methods of vectorized environments.\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m target_envs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_target_envs(indices)\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43menv_i\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_wrapper_attr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmethod_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmethod_kwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m env_i \u001b[38;5;129;01min\u001b[39;00m target_envs]\n",
            "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/gymnasium/wrappers/common.py:409\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is an intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     )\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/gymnasium/core.py:332\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/gymnasium/wrappers/common.py:303\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/gymnasium/envs/toy_text/cliffwalking.py:228\u001b[0m, in \u001b[0;36mCliffWalkingEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/gymnasium/envs/toy_text/cliffwalking.py:315\u001b[0m, in \u001b[0;36mCliffWalkingEnv._render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    313\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m    314\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# rgb_array\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[1;32m    318\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_surface)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    319\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Load the pre-trained model\n",
        "\n",
        "model = DQN.load(model_path)\n",
        "\n",
        "# Initialize the environment\n",
        "\n",
        "env = gym.make(\"CliffWalking-v0\", render_mode=\"human\")\n",
        "\n",
        "# Wrap the environment in DummyVecEnv to make it compatible with SB3\n",
        "\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "# Start a new episode\n",
        "\n",
        "obs = env.reset()\n",
        "done = False\n",
        "\n",
        "# Run the interaction loop\n",
        "\n",
        "while not done:\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    print(obs, reward, done, info)\n",
        "    env.render()\n",
        "\n",
        "# Close the environment\n",
        "\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31f4d39f",
      "metadata": {},
      "source": [
        "### ü§î Not Satisfied with Your Agent's Behavior?\n",
        "\n",
        "Don't worry ‚Äî you have options:\n",
        "\n",
        "- üèãÔ∏è‚Äç‚ôÇÔ∏è **Train longer** ‚Üí Try increasing the number of episodes to improve performance.\n",
        "- üì¶ **Or load our pre-trained model** ‚Üí  \n",
        "  We trained one for **500,000 episodes** ‚Äî uncomment and run the cell below to download it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  139k  100  139k    0     0   188k      0 --:--:-- --:--:-- --:--:--  188k\n"
          ]
        }
      ],
      "source": [
        "!curl https://d37p7d5kaxknzw.cloudfront.net/projects/best_dqn_cliffwalking.zip > best_dqn_cliffwalking.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a01526f",
      "metadata": {},
      "source": [
        "### ‚ùÑÔ∏è Final Note: Understanding `is_slippery=True`\n",
        "\n",
        "You may have wondered what the `is_slippery` parameter does while reading the documentation.\n",
        "\n",
        "Setting `is_slippery=True` in `CliffWalking` adds randomness to the agent's actions ‚Äî introducing a chance that the agent will slip and move in an unintended direction. Training with `is_slippery=True` creates agents that can handle uncertainty ‚Äî a critical skill for real-world applications.\n",
        "\n",
        "#### üéØ Why add stochasticity?\n",
        "\n",
        "- **üåç Realism** ‚Üí Simulates the uncertainty found in real-world environments.  \n",
        "- **üõ°Ô∏è Robustness** ‚Üí Helps agents learn more reliable, adaptable strategies.  \n",
        "- **üî• Challenge** ‚Üí Makes the task more difficult and more interesting to solve.\n",
        "\n",
        "#### ü§î So why didn't we set it to `True` here?\n",
        "\n",
        "The DQN algorithm is designed for simple use cases. It struggles with the high stochasticity that `slippery=True` adds to the `CliffWalking` environment. Think about it: in this simple environment, not following the intended direction immediately means going in a completely different direction: 90 degrees, or even 180 degrees! It would be very difficult for a simple algorithm to learn."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fd63bd6",
      "metadata": {},
      "source": [
        "### Congratulations on completing the project üèÅ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "reinforcement-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
